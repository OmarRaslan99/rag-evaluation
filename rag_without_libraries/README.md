# Mini RAG + Ã‰valuation â€œfrom scratchâ€ (RGPD)

Ce projet montre **comment construire un petit pipeline RAG** (Retrieval-Augmented Generation) **et lâ€™Ã©valuer sans bibliothÃ¨ques dâ€™Ã©valuation externes**, en deux phases :

1. **Phase 1** â€” DonnÃ©es Â« codÃ©es en dur Â» (jouet pÃ©dagogique)
2. **Phase 2** â€” **Dataset RGPD** Ã  partir dâ€™un PDF (articles 1â€“19), **chunking + CSV + chat + Ã©valuation**.

Lâ€™objectif est pÃ©dagogique : comprendre les **briques minimales** dâ€™un RAG et les **mÃ©triques** de base cÃ´tÃ© retrieval et gÃ©nÃ©ration.

---

## ğŸ”§ FonctionnalitÃ©s

* **Extraction PDF** (RGPD\_2.pdf) â†’ **chunking** avec overlap
* **Embeddings** (documents, requÃªtes, rÃ©ponses)
* **Retrieval top-k** par **similaritÃ© cosinus**
* **GÃ©nÃ©ration** de rÃ©ponses avec un LLM (confinÃ© aux passages fournis)
* **Construction dâ€™un dataset CSV** : `context`, `question`, `reponse_attendue`, `reponse_obtenue`, `context_idx`
* **Auto-gÃ©nÃ©ration Q/A** par chunk via LLM (rÃ©ponses courtes, factuelles)
* **Chat en terminal** : pose ta question â†’ RAG rÃ©pond â†’ **Ã©valuation** et **mise Ã  jour du CSV**
* **MÃ©triques from-scratch** :

  * Retrieval : **Precision\@k**, **Recall\@k**, **MRR**, **RelevancyAvg**
  * GÃ©nÃ©ration : **AnswerRelevancy**, **Faithfulness**, **Hallucination**, + **SimilaritÃ© RÃ©ponseâ†”RÃ©ponseAttendue**

---

## ğŸ§± Architecture (vue dâ€™ensemble)

```
PDF RGPD (articles 1â€“19)
        â”‚
        â–¼
   Extraction texte
        â”‚
        â–¼
   Chunking (overlap) â”€â”€â–º Embeddings des chunks
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º GÃ©nÃ©ration Q/A par LLM â†’ CSV (dataset)
        â”‚
        â–¼
   Mode Chat (terminal)
        â”‚
   (Query utilisateur) â”€â–º Embedding query â”€â–º Top-k chunks â”€â–º Prompt LLM â”€â–º RÃ©ponse
        â”‚                                                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã‰valuation â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      MAJ de la colonne `reponse_obtenue` dans le CSV
```

---

## ğŸ—‚ï¸ Fichiers et donnÃ©es

* **`main_phase_1.py`** : script de la phase 1 avec la data codÃ©e en dure
* **`main_phase_2.py`** : script principal (extraction, dataset, chat, Ã©valuation)
* **`RGPD_2.pdf`** : PDF source (articles 1â€“19 du RGPD)
* **`rgpd_dataset.csv`** : dataset gÃ©nÃ©rÃ©, colonnes :

  * `id` : identifiant
  * `context` : le chunk de texte
  * `question` : question gÃ©nÃ©rÃ©e (LLM)
  * `reponse_attendue` : rÃ©ponse gÃ©nÃ©rÃ©e (LLM) pour ce chunk
  * `reponse_obtenue` : rÃ©ponse du systÃ¨me lors du chat
  * `context_idx` : index du chunk source

---

## âœ… PrÃ©-requis

* Python 3.10+
* ClÃ© OpenAI dans `.env`

### Installation des dÃ©pendances

```bash
pip install langchain-openai python-dotenv numpy pypdf
```

### Variables dâ€™environnement (fichier `.env`)

```env
OPENAI_API_KEY=sk-...
OPENAI_CHAT_MODEL=gpt-4o
RGPD_PDF_PATH=RGPD_2.pdf
RGPD_CSV_PATH=rgpd_dataset.csv
RAG_TOPK=3
```

---

## â–¶ï¸ Lancer le projet

```bash
python main_phase_2.py
```
(ou `python main_phase_1.py` pour tester la phase 1)

* **Premier run** :

  1. Lit `RGPD_2.pdf`, **chunk** le texte
  2. **GÃ©nÃ¨re** des paires **Question/RÃ©ponse\_attendue** (LLM)
  3. **CrÃ©e**/met Ã  jour `rgpd_dataset.csv`
  4. Lance le **mode chat** (terminal)

* **Runs suivants** :
  RÃ©utilise le CSV existant et recharge les chunks depuis le PDF.

---

## ğŸ¤– Mode Chat (terminal)

* Tape une **question** sur le RGPD
* Le systÃ¨me :

  1. Trouve les **top-k** chunks pertinents
  2. **GÃ©nÃ¨re une rÃ©ponse** basÃ©e **uniquement** sur ces chunks
  3. Cherche dans le CSV la **question la plus proche** (embeddings)
  4. **Ã‰value** (retrieval + gÃ©nÃ©ration) vs `reponse_attendue`
  5. **Sauvegarde** la rÃ©ponse gÃ©nÃ©rÃ©e dans `reponse_obtenue` (CSV)

*Quitter* : `q`, `quit`, `exit`.

---

## ğŸ“ MÃ©triques (explications simples)

### CÃ´tÃ© **Retrieval**

* **Precision\@k** : parmi les `k` passages ramenÃ©s, **quelle part est pertinente** ?
* **Recall\@k** : parmi **tous** les passages pertinents existants, **combien** sont dans le top-k ?
* **MRR** (Mean Reciprocal Rank) : **rÃ©compense** les passages pertinents **bien classÃ©s** (haut de liste).
* **RelevancyAvg** : **moyenne** des similaritÃ©s cosinus (query â†” chacun des top-k passages).

### CÃ´tÃ© **GÃ©nÃ©ration**

* **AnswerRelevancy** : similaritÃ© **question â†” rÃ©ponse** (la rÃ©ponse reste-t-elle *sur le sujet* ?)
* **Faithfulness** : similaritÃ© **rÃ©ponse â†” contexte** (la rÃ©ponse est-elle *ancrÃ©e* dans les passages ?)
* **Hallucination** : `1 âˆ’ Faithfulness` (*moins câ€™est Ã©levÃ©, mieux câ€™est*)
* **Sim(RÃ©ponse â†” RÃ©ponseAttendue)** : proximitÃ© **pratique** entre ce qui est gÃ©nÃ©rÃ© et la **rÃ©ponse attendue** du dataset.

> **SimilaritÃ© cosinus** : mesure lâ€™**angle** entre deux vecteurs dâ€™embedding (de âˆ’1 Ã  +1). Plus câ€™est proche de **+1**, plus les textes se ressemblent sÃ©mantiquement.

---

## âš™ï¸ ParamÃ¨tres utiles (dans `main()`)

* `chunk_size` / `overlap` : granularitÃ© et recouvrement des chunks
* `max_pairs_per_chunk` : nombre de Q/A gÃ©nÃ©rÃ©es par chunk
* `max_chunks` : limite le nombre de chunks utilisÃ©s pour construire le dataset
* `RAG_TOPK` (env) : top-k des documents rÃ©cupÃ©rÃ©s

---

## ğŸªœ Ã‰tapes internes (dÃ©tails)

1. **Extraction PDF** : `read_pdf_text()` (via `pypdf.PdfReader`)
2. **Nettoyage/Chunking** : `clean_text()` + `chunk_text()`
3. **Embeddings** : `OpenAIEmbeddings().embed_documents()` / `.embed_query()`
4. **Retrieval** : **cosinus** query â†” chunks, tri dÃ©croissant, top-k
5. **GÃ©nÃ©ration** : prompt **(question + top-k)** â†’ `ChatOpenAI(model)`
6. **Dataset CSV** :

   * **CrÃ©ation** : `build_dataset_from_pdf()` â†’ appelle `llm_generate_qa_for_chunk()`
   * **Mise Ã  jour** : `overwrite_dataset()` aprÃ¨s chaque question utilisateur
7. **Ã‰valuation** :

   * Retrieval â†’ `compute_retrieval_metrics()`
   * GÃ©nÃ©ration â†’ `compute_generation_metrics()`

---

## ğŸ§© Limites & choix pÃ©dagogiques

* **Parsing Q/A simplifiÃ©** (format Â« Q: â€¦ / A: â€¦ Â»)
* **Ground truth minimal** : par dÃ©faut, on considÃ¨re au moins le `context_idx` de la question la plus proche comme pertinent
* **SimilaritÃ©s â‰  vÃ©ritÃ© absolue** : utiles pour comparer et progresser, mais restent des approximations
* **LLM** : gÃ©nÃ¨re aussi les questions/rÃ©ponses attendues du dataset â†’ bien calibrer le *prompting* pour limiter lâ€™invention

---

## ğŸ› ï¸ Personnaliser / Ã‰tendre

* Multiplier les **Q/A par chunk** (`max_pairs_per_chunk > 1`)
* Ajouter dâ€™autres **mÃ©triques** (nDCG, MAP, etc.)
* Introduire un **judge LLM** optionnel pour la foi/contradiction (hors â€œfrom scratchâ€)
* Remplacer lâ€™**embedding** par un autre modÃ¨le si besoin
* Persister les **embeddings** (fichier / base) pour accÃ©lÃ©rer

---

## â“DÃ©pannage rapide

* **CSV vide ?** Premier run normal. VÃ©rifie `RGPD_PDF_PATH`, droits de lecture, contenu du PDF.
* **Pas de rÃ©ponses pertinentes ?** Augmente `chunk_size`, ajuste `overlap`, ou `RAG_TOPK`.
* **Hallucination Ã©levÃ©e ?** Renforce le message systÃ¨me (Â« rÃ©pondre uniquement Ã  partir des extraits Â»), rÃ©duis le bruit des chunks.
* **Precision/Recall Ã  0 alors quâ€™un chunk semble correct ?** VÃ©rifie que le **ground truth** correspond bien Ã  lâ€™index (`context_idx`) de la ligne ciblÃ©e.

---


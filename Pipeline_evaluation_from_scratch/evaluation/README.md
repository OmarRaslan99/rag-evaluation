# Projet 2 : `chat_eval.py` ‚Äî √âvaluation auto (sans chat) + Logs CSV

## 1) Objet

Charger un **dataset** (CSV/JSON) de contexts + Q/R attendues, **s√©lectionner N questions al√©atoires du dataset**, ex√©cuter un **RAG minimal** :

* Retrieval top-k (cosinus)
* G√©n√©ration LLM **contraint aux extraits**
* **√âvaluation** par question (Precision@k, Recall@k, MRR, RelevancyAvg, AnswerRel, Faithfulness, Hallucination, ExpectedSim)
* **Journalisation** dans un **CSV de logs** (1 ligne par question)

> üîÅ Plus de mode chat ni de saisie utilisateur : tout est automatique √† partir des Q/R du dataset.

## 2) Pr√©requis

* Python 3.10+
* Cl√© OpenAI
* D√©pendances :

```bash
pip install python-dotenv langchain-openai numpy
```

## 3) Configuration

Cr√©er un fichier `.env` :

```env
OPENAI_API_KEY=sk-...
OPENAI_CHAT_MODEL=gpt-4o
RAG_TOPK=3
```

Tu peux copier le contenu de `.env.example` vers `.env` puis ajuster.

## 4) Formats d‚Äôentr√©e

### CSV (recommand√©)

Colonnes attendues (m√™mes que Projet 1) :
`id, context, question, reponse_attendue, reponse_obtenue, context_idx`

### JSON (alternatif)

Liste d‚Äôobjets contenant les m√™mes cl√©s :

```json
[
  {
    "id": 1,
    "context": "Texte du chunk...",
    "question": "Question bas√©e sur ce chunk ?",
    "reponse_attendue": "R√©ponse courte et factuelle.",
    "reponse_obtenue": "",
    "context_idx": 0
  }
]
```

## 5) Utilisation

### Exemple (5 questions al√©atoires, reproductible)

```bash
python chat_eval.py \
  --dataset page_impact_social_societal_dataset.csv \
  --format csv \
  --out logs_chat_eval.csv \
  --topk 3 \
  --num-questions 5 \
  --seed 42
```

### Arguments

* `--dataset` (**obligatoire**) : chemin du dataset (CSV/JSON)
* `--format` (**obligatoire**) : `csv` ou `json`
* `--out` (**obligatoire**) : chemin du CSV de **logs** (sera cr√©√© si absent)
* `--topk` *(def: `RAG_TOPK` ou 3)* : nb de documents r√©cup√©r√©s
* `--num-questions` *(def: 5)* : nb de questions tir√©es al√©atoirement dans le dataset
  (si le dataset contient < N questions, toutes seront utilis√©es)
* `--seed` *(optionnel)* : graine al√©atoire pour un √©chantillonnage reproductible

## 6) Sch√©ma du CSV de logs (sortie)

| Colonne             | Description                                                                          |
| ------------------- | ------------------------------------------------------------------------------------ |
| `id`                | Identifiant de la ligne de log                                                       |
| `timestamp`         | Horodatage ISO (UTC)                                                                 |
| `question`          | Question (issue du dataset)                                                          |
| `answer`            | R√©ponse du LLM                                                                       |
| `retrieved_indices` | JSON list des indices des chunks r√©cup√©r√©s (ex: `[12, 11, 13]`)                      |
| `retrieved_texts`   | Concat√©nation des extraits utilis√©s (s√©par√©s par `---`)                              |
| `evaluation_json`   | JSON compact des m√©triques (precision, recall, mrr, relevancy_avg, answer_rel, etc.) |

## 7) D√©tails de l‚Äô√©valuation

* **S√©lection des questions** : tirage al√©atoire dans le dataset (`--num-questions`, optionnellement fix√© par `--seed`).
* **Ground truth** : bas√© **directement sur la ligne tir√©e** (donc sur son `context_idx`), puis **¬±1** autour de ce chunk.
* **Retrieval** :

  * *Precision@k* = pertinents dans le top-k / k
  * *Recall@k* = pertinents retrouv√©s / pertinents totaux
  * *MRR* = moyenne des 1/rang sur les pertinents retrouv√©s
  * *RelevancyAvg* = moyenne des cosinus des top-k
* **G√©n√©ration** :

  * *AnswerRel* (Q ‚Üî A, cosinus embeddings)
  * *Faithfulness* (A ‚Üî moyenne des embeddings des contexts top-k)
  * *Hallucination* = 1 ‚àí Faithfulness
  * *ExpectedSim* (A ‚Üî `reponse_attendue` du dataset)

## 8) Bonnes pratiques

* Utilise `--seed` pour des runs reproductibles lorsqu‚Äôil y a du sampling.
* Harmonise la granularit√© : le **chunking** du dataset doit correspondre aux **docs** charg√©s pour le retrieval.
* Pour r√©duire les co√ªts API : limite `--num-questions` et contr√¥le `--topk`.

---
